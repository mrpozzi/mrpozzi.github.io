---
layout: post
title: "ML 101"
date: 2018-03-15 8:30:00
categories: nlp
description: Machine Learning 101: 
image: /images/lda_graphical_model.png
---



* Unsupervised
* Supervised
	* Regression
	* Classification


ML Model

Machine Learning Model
Binary Classification
Evaluate a Classifier
Lift & Gain
Take Home & Conclusions

[Black Box]


Many ways to compare regressions (much harder to find a tradeoff...)

Predicting a Binary Label:
Won/Lost
Spam/Not Spam
Cancer/Not Cancer
WAY Simpler: youâ€™re either Right or Wrong!

Logistic Regression
(Ensembles of) Decision Trees. E.g. Random Forest, GBT
Support Vector Machines


Decision regions... shit gets weird...


One Metric to Rule Them All!


Decision matrix!


---+---
TP | FN
---+---
FP | FN
---+---

Usual suspects

Accuracy: (TP + TN) / TOTAL
Precision: TP / (TP + FP)                              (higher => fewer false positives.)
Sensitivity (a.k.a. Recall): TP / (TP + FN)     (higher => fewer false negatives.)




Shades of Grey

To each point we attach a score, not a label
The score is monotonic in the probability of having label 1


The threshold choice is somewhat subjective and it depends on the tradeoff between False Positives and False  Negatives:
* Marketing
* Fraud Detection
* Diagnostic Tools



Label 1 if score > t
All the metrics change with the threshold.

---+---
TP(t) | FN(t)
---+---
FP(t) | FN(t)
---+---


ROC curve
TPR = TP / (TP + FN)  a.k.a. Sensitivity
vs. 
FPR = FP / (FP + TN)       (1 - Precision)




Take Home points:
Classification and Scoring.
Practical considerations 
Visualizations.

